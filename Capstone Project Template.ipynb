{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project will combine data from various sources, clean, combine, and store them in a way that can be used for either analysis or source of truth database.  The project will make use of AWS S3 and EMR cluster for storage and computing.  The raw data will be stored on S3 where the EMR cluster will pull data and save a datalake and data that can be used for a data warehouse.  All the steps  and reasoning will be outlined in this notebook.  \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import isnan, when, count, col, regexp_replace, lower,trim, to_date\n",
    "from pyspark.sql.types import DoubleType,IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "- In this project we will combine airport, immigration, city demographic, and temperature data to provide immigrants and officials a source of truth database or analysis. \n",
    "\n",
    "- Here we will work with a subset of the immigration data set to plan our project due to the size of the data.  The full data set will be used when running the ETL on the EMR cluster.\n",
    "\n",
    "- In order to reduce hardware requirements to work with the large data set we will use spark rather than something like pandas which would require more resources.\n",
    "\n",
    "- We want a solution that we can work with that is not confined to a network or personal computer so any raw data that is on the udacity workspace, or minor table that is created for category conversions, will be moved to AWS S3.\n",
    "\n",
    "- There will be two end solutions, a data lake and a data warehouse.\n",
    "    - Data lake: This will contain data based on each immigrant with their associated state demographic/temperature data and airport code data if any connection exists. \n",
    "    - Data warehouse: This will contain 3 different data sets.  State data containing demographic and temperature data for each state.  Airport code data containing some general information on each airport.  Immigraiton data will contain I-94 arrivals data collected by the NTTO(National Travel and Tourism Office)\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "- Airport code data\n",
    "    - This data contains information on airports in the United States mostly around location.\n",
    "    - The data comes from: https://datahub.io/core/airport-codes#data\n",
    "- Immigration data\n",
    "    - This data comes from the US National Tourism and Trade Office, and \n",
    "    - The data comes from: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "- Immigration supporting data:\n",
    "    - All supporting data comes from the `i94_SAS_Labels_Descriptions.SAS` document included.\n",
    "    - i94cit_res_codes\n",
    "        - This contains codes for city and city of residence conversion: 3 digits to city\n",
    "    - i94port_codes\n",
    "        - This contains the port conversion codes: 3 letter to city/state\n",
    "    - Visa data\n",
    "        - This data contains conversions from numeric to categorical visa (business, pleasure, student).\n",
    "    - Mode data\n",
    "        - This data contains conversions from numeric to categorical mode of entry(land, sea, air).\n",
    "    - i94addr\n",
    "        - This contains state abbreviations to state names.        \n",
    "            - All data comes from the `i94_SAS_Labels_Descriptions.SAS` document included.\n",
    "\n",
    "- City demographic data\n",
    "    - This data contails demographic information on cities in the United States.\n",
    "    - This data comes from: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "- Temperature data\n",
    "    - This data contains temperature data from around the world on each city as far back as 1750.\n",
    "    - This is a kaggle data set and comes from: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### This cell does not need to be run, All relevent files have been moved to S3\n",
    "# Uncomment to run \n",
    "\n",
    "#config = configparser.ConfigParser()\n",
    "#config.read_file(open('dl.cfg'))\n",
    "\n",
    "# Set AWS key and secret key\n",
    "#os.environ['AWS_ACCESS_KEY_ID'] = config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "#KEY = config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "#SECRET = config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "# upload files to s3\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "#upload_file(file_name='airport-codes_csv.csv' ,bucket='de-capstone',object_name='raw_data/airport-codes_csv.csv')\n",
    "#upload_file(file_name='i94addr.csv' ,bucket='de-capstone',object_name='raw_data/i94addr.csv')\n",
    "#upload_file(file_name='i94cit_res_codes.csv' ,bucket='de-capstone',object_name='raw_data/i94cit_res_codes.csv')\n",
    "#upload_file(file_name='i94port_code.csv' ,bucket='de-capstone',object_name='raw_data/i94port_code.csv')\n",
    "#upload_file(file_name='us-cities-demographics.csv' ,bucket='de-capstone',object_name='raw_data/us-cities-demographics.csv')\n",
    "#upload_file(file_name='../../data2/GlobalLandTemperaturesByCity.csv' ,bucket='de-capstone',object_name='raw_data/temp_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This cell is a test column for reading and writing the immigration data\n",
    "\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "- Identify data quality issues, like missing values, duplicate data, etc.\n",
    "- Set correct data types for relevant fields\n",
    "- Transform/string split necessary fields depending on use case.\n",
    "- Condense tables to usable scale.\n",
    "\n",
    "#### Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Code data\n",
    "- Airport code data will be used to create a table about airports in the US\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|       NA|         US|     US-FL|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|       NA|         US|     US-AK|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|       NA|         US|     US-CO|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515|       NA|         US|     US-TX| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       NA|         US|     US-MA|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "df_apt_codes = spark.read.csv('airport-codes_csv.csv',header=True)\n",
    "\n",
    "# pull just the airports in the US\n",
    "df_apt_codes = df_apt_codes.filter(df_apt_codes.iso_country=='US')\n",
    "\n",
    "# remove iata code == null\n",
    "df_apt_codes = df_apt_codes.filter(df_apt_codes.iata_code.isNotNull())\n",
    "\n",
    "#check data\n",
    "df_apt_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|    municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------------+--------+---------+----------+--------------------+\n",
      "|  ESP|       closed|Birchwood-Pocono ...|         965|       NA|         US|     US-PA|East Stroudsburg|    null|      ESP|      null|   -75.2521, 41.0643|\n",
      "| KN53|small_airport|Stroudsburg Pocon...|         480|       NA|         US|     US-PA|East Stroudsburg|    KN53|      ESP|       N53|-75.1605987549, 4...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+----------------+--------+---------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check a IATA code\n",
    "df_apt_codes.where(df_apt_codes.iata_code == 'ESP').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get state code from iso_region\n",
    "df_apt_codes = df_apt_codes.withColumn('State_Code',split(df_apt_codes.iso_region,'-').getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|State_Code|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|       NA|         US|     US-FL|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|        FL|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|       NA|         US|     US-AK|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|        AK|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|       NA|         US|     US-CO|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|        CO|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515|       NA|         US|     US-TX| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|        TX|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       NA|         US|     US-MA|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|        MA|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apt_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|State_Code|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "|    0|   0|   0|          34|        0|          0|         0|           6|      81|        0|        50|          0|         0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "df_apt_codes.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_apt_codes.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "df_temp = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv',header=True)\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check amount of rows\n",
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'string'),\n",
       " ('AverageTemperature', 'string'),\n",
       " ('AverageTemperatureUncertainty', 'string'),\n",
       " ('City', 'string'),\n",
       " ('Country', 'string'),\n",
       " ('Latitude', 'string'),\n",
       " ('Longitude', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "df_temp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Change AverageTemperature to Double\n",
    "df_temp = df_temp.withColumn(\"AverageTemperature\", df_temp[\"AverageTemperature\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687289"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check amount for United states\n",
    "df_temp.filter(df_temp.Country == 'United States').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter just United states\n",
    "df_temp = df_temp.filter(df_temp.Country == 'United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|             25765|                        25765|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for nulls\n",
    "df_temp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_temp.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop nulls\n",
    "df_temp = df_temp.where(col('AverageTemperature').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|                 0|                            0|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_temp.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------+------------------+\n",
      "|            City|Latitude|Longitude|AverageTemperature|\n",
      "+----------------+--------+---------+------------------+\n",
      "|      Saint Paul|  45.81N|   93.46W| 4.946022285896202|\n",
      "|        Torrance|  34.56N|  118.70W|15.878038442083941|\n",
      "|     Clarksville|  36.17N|   87.51W|14.281885540237276|\n",
      "|          Rialto|  34.56N|  116.76W| 17.05575872534143|\n",
      "|          Denver|  39.38N|  104.05W| 8.777836262323191|\n",
      "|     Little Rock|  34.56N|   91.46W|16.382053970436562|\n",
      "|      Providence|  42.59N|   72.00W| 7.341440525809558|\n",
      "|       Arlington|  32.95N|   96.70W|18.062719999999995|\n",
      "|       Arlington|  39.38N|   76.99W|11.918474511061234|\n",
      "|         Madison|  42.59N|   89.45W| 7.999159821712816|\n",
      "|          Fresno|  36.17N|  119.34W|15.817692463328274|\n",
      "|Port Saint Lucie|  26.52N|   80.60W| 23.06892444289695|\n",
      "|          Pueblo|  37.78N|  103.73W|10.730368330464712|\n",
      "|      Chesapeake|  36.17N|   75.58W|15.678856043603744|\n",
      "|      Des Moines|  40.99N|   93.73W| 9.857171601400834|\n",
      "|   Moreno Valley|  34.56N|  116.76W| 17.05575872534143|\n",
      "|  Pembroke Pines|  26.52N|   80.60W| 23.06892444289695|\n",
      "|      Richardson|  32.95N|   96.70W|18.062719999999995|\n",
      "|          Toledo|  40.99N|   83.08W| 9.592642193010553|\n",
      "|      Evansville|  37.78N|   87.46W| 13.47039083039434|\n",
      "+----------------+--------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get average temp for each city\n",
    "avg_temp = df_temp.cube(['City','Latitude','Longitude']).agg({'AverageTemperature':'avg'})\n",
    "avg_temp = avg_temp.na.drop()\n",
    "avg_temp = avg_temp.withColumnRenamed('avg(AverageTemperature)', 'AverageTemperature')\n",
    "avg_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Because Immigration data is from 2016 and the latest temperature data is 2013 we will take the average temp of each city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data\n",
    "We will match demographic data to an individuals `i94addr`. This is a state value so we will need to condense the demographic data down to the state level.  While doing this we will also introduce average temperature data.  We will match temperature on city, then collapse it down to the state level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+------------------+\n",
      "|      City|    State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|AverageTemperature|\n",
      "+----------+---------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+------------------+\n",
      "|Saint Paul|Minnesota|      31.5|         149547|           151293|          300840|             10548|       56514|                  2.58|        MN|American Indian a...|  6858| 4.946022285896202|\n",
      "|Saint Paul|Minnesota|      31.5|         149547|           151293|          300840|             10548|       56514|                  2.58|        MN|Black or African-...| 54665| 4.946022285896202|\n",
      "|Saint Paul|Minnesota|      31.5|         149547|           151293|          300840|             10548|       56514|                  2.58|        MN|               White|191369| 4.946022285896202|\n",
      "|Saint Paul|Minnesota|      31.5|         149547|           151293|          300840|             10548|       56514|                  2.58|        MN|               Asian| 58174| 4.946022285896202|\n",
      "|Saint Paul|Minnesota|      31.5|         149547|           151293|          300840|             10548|       56514|                  2.58|        MN|  Hispanic or Latino| 27307| 4.946022285896202|\n",
      "+----------+---------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "df_demo = spark.read.csv('us-cities-demographics.csv',sep = ';',header=True)\n",
    "\n",
    "# Combine temp data\n",
    "df_demo = df_demo.join(avg_temp,avg_temp.City==df_demo.City).select(df_demo[\"*\"],avg_temp[\"AverageTemperature\"])\n",
    "df_demo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+\n",
      "|      State|State Code|AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|\n",
      "+-----------+----------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+\n",
      "|  Wisconsin|        WI| 8.394430860660089|       4771655.0|        2464515.0|          179205.0|    491120.0|      2307140.0|    2.3699999999999997|\n",
      "|   Oklahoma|        OK|15.616847364286048|       5773255.0|        2942520.0|          360465.0|    663105.0|      2830735.0|    2.4733333333333336|\n",
      "|Connecticut|        CT|10.896842198400124|       3640550.0|        1883690.0|           94910.0|    925555.0|      1756860.0|    2.6733333333333333|\n",
      "|   Colorado|        CO| 8.425446029673148|      1.372057E7|        6923450.0|          914575.0|   1753415.0|      6797120.0|    2.5854545454545454|\n",
      "|     Nevada|        NV| 16.58674593979649|      1.004586E7|        5037265.0|          686030.0|   2172370.0|      5008595.0|     2.828571428571429|\n",
      "+-----------+----------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Break data down to state level\n",
    "state_df = df_demo.cube('State','State Code').agg({'Male Population':'sum',\n",
    "                                     'Female Population':'sum',\n",
    "                                     'Total Population':'sum',\n",
    "                                     'Number of Veterans':'sum',\n",
    "                                     'Foreign-born':'sum',\n",
    "                                     'Average Household Size':'avg',\n",
    "                                     'AverageTemperature':'avg'})\n",
    "state_df = state_df.na.drop()\n",
    "state_df = state_df.withColumnRenamed('avg(AverageTemperature)', 'AverageTemperature')\n",
    "state_df = state_df.withColumnRenamed('avg(Average Household Size)', 'Average Household Size')\n",
    "state_df = state_df.withColumnRenamed('sum(Total Population)', 'Total Population')\n",
    "state_df = state_df.withColumnRenamed('sum(Female Population)', 'Female Population')\n",
    "state_df = state_df.withColumnRenamed('sum(Male Population)', 'Male Population')\n",
    "state_df = state_df.withColumnRenamed('sum(Number of Veterans)', 'Number of Veterans')\n",
    "state_df = state_df.withColumnRenamed('sum(Foreign-born)', 'Foreign-born')\n",
    "\n",
    "\n",
    "\n",
    "state_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Race|\n",
      "+--------------------+\n",
      "|Black or African-...|\n",
      "|  Hispanic or Latino|\n",
      "|               White|\n",
      "|               Asian|\n",
      "|American Indian a...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.select('Race').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(State='Rhode Island', American Indian and Alaska Native=4171.0),\n",
       " Row(State='District of Columbia', American Indian and Alaska Native=6130.0),\n",
       " Row(State='Connecticut', American Indian and Alaska Native=9643.0),\n",
       " Row(State='Missouri', American Indian and Alaska Native=20469.0),\n",
       " Row(State='Utah', American Indian and Alaska Native=11466.0),\n",
       " Row(State='Arizona', American Indian and Alaska Native=120750.0),\n",
       " Row(State=None, American Indian and Alaska Native=1397283.0),\n",
       " Row(State='Tennessee', American Indian and Alaska Native=15899.0),\n",
       " Row(State='New Jersey', American Indian and Alaska Native=7755.0),\n",
       " Row(State='Maine', American Indian and Alaska Native=662.0),\n",
       " Row(State='Wisconsin', American Indian and Alaska Native=19656.0),\n",
       " Row(State='Oklahoma', American Indian and Alaska Native=94626.0),\n",
       " Row(State='Massachusetts', American Indian and Alaska Native=15532.0),\n",
       " Row(State='Illinois', American Indian and Alaska Native=36446.0),\n",
       " Row(State='Kansas', American Indian and Alaska Native=19480.0),\n",
       " Row(State='Ohio', American Indian and Alaska Native=40291.0),\n",
       " Row(State='Michigan', American Indian and Alaska Native=19656.0),\n",
       " Row(State='Nevada', American Indian and Alaska Native=30594.0),\n",
       " Row(State='Alaska', American Indian and Alaska Native=36339.0),\n",
       " Row(State='Oregon', American Indian and Alaska Native=28027.0),\n",
       " Row(State='Maryland', American Indian and Alaska Native=8218.0),\n",
       " Row(State='New York', American Indian and Alaska Native=104903.0),\n",
       " Row(State='New Hampshire', American Indian and Alaska Native=558.0),\n",
       " Row(State='Colorado', American Indian and Alaska Native=63143.0),\n",
       " Row(State='Nebraska', American Indian and Alaska Native=10599.0),\n",
       " Row(State='Indiana', American Indian and Alaska Native=13357.0),\n",
       " Row(State='California', American Indian and Alaska Native=303433.0),\n",
       " Row(State='Texas', American Indian and Alaska Native=129891.0),\n",
       " Row(State='South Dakota', American Indian and Alaska Native=4623.0),\n",
       " Row(State='Arkansas', American Indian and Alaska Native=3019.0),\n",
       " Row(State='Iowa', American Indian and Alaska Native=5207.0),\n",
       " Row(State='Pennsylvania', American Indian and Alaska Native=20189.0),\n",
       " Row(State='Florida', American Indian and Alaska Native=28018.0),\n",
       " Row(State='Mississippi', American Indian and Alaska Native=323.0),\n",
       " Row(State='Louisiana', American Indian and Alaska Native=6883.0),\n",
       " Row(State='Alabama', American Indian and Alaska Native=7167.0),\n",
       " Row(State='Georgia', American Indian and Alaska Native=9408.0),\n",
       " Row(State='Minnesota', American Indian and Alaska Native=17268.0),\n",
       " Row(State='North Carolina', American Indian and Alaska Native=29855.0),\n",
       " Row(State='Washington', American Indian and Alaska Native=32035.0),\n",
       " Row(State='New Mexico', American Indian and Alaska Native=32243.0),\n",
       " Row(State='Virginia', American Indian and Alaska Native=27298.0),\n",
       " Row(State='South Carolina', American Indian and Alaska Native=2053.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up Races for each state\n",
    "white_df = df_demo.where(df_demo.Race =='White').cube('State').agg({'Count':'sum'})\n",
    "white_df = white_df.withColumnRenamed('sum(Count)', 'White')\n",
    "\n",
    "asian_df = df_demo.where(df_demo.Race =='Asian').cube('State').agg({'Count':'sum'})\n",
    "asian_df = asian_df.withColumnRenamed('sum(Count)', 'Asian')\n",
    "\n",
    "black_df = df_demo.where(df_demo.Race =='Black or African-American').cube('State').agg({'Count':'sum'})\n",
    "black_df = black_df.withColumnRenamed('sum(Count)', 'Black or African-American')\n",
    "\n",
    "hispanic_df = df_demo.where(df_demo.Race =='Hispanic or Latino').cube('State').agg({'Count':'sum'})\n",
    "hispanic_df = hispanic_df.withColumnRenamed('sum(Count)', 'Hispanic or Latino')\n",
    "\n",
    "native_df = df_demo.where(df_demo.Race =='American Indian and Alaska Native').cube('State').agg({'Count':'sum'})\n",
    "native_df = native_df.withColumnRenamed('sum(Count)', 'American Indian and Alaska Native')\n",
    "\n",
    "white_df.collect()\n",
    "asian_df.collect()\n",
    "black_df.collect()\n",
    "hispanic_df.collect()\n",
    "native_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Combine race data\n",
    "state_df = state_df.join(white_df,white_df.State==state_df.State).select(state_df[\"*\"],white_df[\"White\"])\n",
    "state_df = state_df.join(asian_df,asian_df.State==state_df.State).select(state_df[\"*\"],asian_df[\"Asian\"])\n",
    "state_df = state_df.join(black_df,black_df.State==state_df.State).select(state_df[\"*\"],black_df[\"Black or African-American\"])\n",
    "state_df = state_df.join(hispanic_df,hispanic_df.State==state_df.State).select(state_df[\"*\"],hispanic_df[\"Hispanic or Latino\"])\n",
    "state_df = state_df.join(native_df,native_df.State==state_df.State).select(state_df[\"*\"],native_df[\"American Indian and Alaska Native\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------+----------------+-----------------+------------------+------------+---------------+----------------------+---------+--------+-------------------------+------------------+---------------------------------+\n",
      "|               State|State Code| AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|    White|   Asian|Black or African-American|Hispanic or Latino|American Indian and Alaska Native|\n",
      "+--------------------+----------+-------------------+----------------+-----------------+------------------+------------+---------------+----------------------+---------+--------+-------------------------+------------------+---------------------------------+\n",
      "|                Utah|        UT|  9.898601558734654|       2780420.0|        1376780.0|           87315.0|    426475.0|      1403640.0|    3.2325000000000004| 447902.0| 30346.0|                  12828.0|          122622.0|                          11466.0|\n",
      "|           Minnesota|        MN| 5.8729834447628075|       4119955.0|        2064815.0|          163265.0|    725230.0|      2055140.0|    2.4633333333333334| 562708.0|101867.0|                 154086.0|           73905.0|                          17268.0|\n",
      "|                Ohio|        OH|  11.33790573901892|      1.501329E7|        7728170.0|          753265.0|   1323230.0|      7285120.0|    2.2728571428571427|1831672.0|133451.0|                1061736.0|          175677.0|                          40291.0|\n",
      "|            Arkansas|        AR| 16.398790531226602|       1404090.0|         709310.0|           85435.0|    114765.0|       694780.0|    2.3199999999999994| 171142.0| 13130.0|                  92533.0|           21035.0|                           3019.0|\n",
      "|              Oregon|        OR|  9.772739535594242|       4800890.0|        2433760.0|          242605.0|    601060.0|      2367130.0|    2.4866666666666664| 819715.0| 81535.0|                  55316.0|          117834.0|                          28027.0|\n",
      "|               Texas|        TX| 18.213994753590246|     5.9507865E7|      3.0036775E7|         2830695.0| 1.2878715E7|     2.947109E7|     2.825714285714286|8598909.0|725535.0|                1907881.0|         5501866.0|                         129891.0|\n",
      "|        Pennsylvania|        PA| 10.730472106444385|       9359135.0|        4904335.0|          398615.0|   1167630.0|      4454800.0|    2.3699999999999997| 896993.0|143948.0|                 773434.0|          228304.0|                          20189.0|\n",
      "|         Connecticut|        CT| 10.896842198400124|       3640550.0|        1883690.0|           94910.0|    925555.0|      1756860.0|    2.6733333333333333| 393326.0| 38664.0|                 213495.0|          254659.0|                           9643.0|\n",
      "|            Nebraska|        NE| 10.051168201978832|       3606165.0|        1819500.0|          195985.0|    356105.0|      1786665.0|                 2.435| 600094.0| 34243.0|                  80668.0|           83812.0|                          10599.0|\n",
      "|              Nevada|        NV|  16.58674593979649|      1.004586E7|        5037265.0|          686030.0|   2172370.0|      5008595.0|     2.828571428571429|1367482.0|211234.0|                 250928.0|          635332.0|                          30594.0|\n",
      "|          Washington|        WA|  7.975538905493793|       7091635.0|        3576385.0|          421105.0|   1206790.0|      3515250.0|    2.3739999999999997|1086644.0|220580.0|                 114604.0|          117588.0|                          32035.0|\n",
      "|            Illinois|        IL| 10.874525164537129|     2.0826325E7|      1.0743215E7|          693265.0|   3871900.0|     1.008311E7|    2.6363636363636362|2355539.0|294589.0|                1139471.0|         1060135.0|                          36446.0|\n",
      "|            Oklahoma|        OK| 15.616847364286048|       5773255.0|        2942520.0|          360465.0|    663105.0|      2830735.0|    2.4733333333333336| 847842.0| 56616.0|                 190936.0|          190311.0|                          94626.0|\n",
      "|District of Columbia|        DC| 11.918474511061234|       3361140.0|        1762615.0|          129815.0|    475585.0|      1598525.0|                  2.24| 285402.0| 35072.0|                 328786.0|           71129.0|                           6130.0|\n",
      "|              Alaska|        AK|-2.3016456107756667|       1493475.0|         728750.0|          137460.0|    166290.0|       764725.0|                  2.77| 212696.0| 36825.0|                  23107.0|           27261.0|                          36339.0|\n",
      "|          New Mexico|        NM|  11.13526393728221|       2795655.0|        1429040.0|          187215.0|    291000.0|      1366615.0|                  2.49| 411847.0| 25140.0|                  26774.0|          271854.0|                          32243.0|\n",
      "|            Missouri|        MO| 11.923398632124142|       7638965.0|        3961370.0|          467055.0|    492620.0|      3677595.0|     2.224285714285714|1102262.0| 55676.0|                 354730.0|           96790.0|                          20469.0|\n",
      "|        Rhode Island|        RI|  7.341440525809558|        896020.0|         450570.0|           24665.0|    267660.0|       445450.0|                  2.72|  97885.0| 13432.0|                  30638.0|           77968.0|                           4171.0|\n",
      "|             Georgia|        GA| 15.469405578711113|       5053585.0|        2598990.0|          358915.0|    315615.0|      2454595.0|                  2.49| 448878.0| 43712.0|                 518406.0|           58747.0|                           9408.0|\n",
      "|            Michigan|        MI|  8.495453752083435|       7213528.0|        3748073.0|          322815.0|    682677.0|      3465455.0|    2.4526470588235294| 667709.0| 64903.0|                 724599.0|          113941.0|                          19656.0|\n",
      "+--------------------+----------+-------------------+----------------+-----------------+------------------+------------+---------------+----------------------+---------+--------+-------------------------+------------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94cit_res_codes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------+---------------+\n",
      "|i94cit|       i94_City|i94res|   i94_Resident|\n",
      "+------+---------------+------+---------------+\n",
      "| 582.0|         MEXICO| 582.0|         MEXICO|\n",
      "| 236.0|    AFGHANISTAN| 236.0|    AFGHANISTAN|\n",
      "| 101.0|        ALBANIA| 101.0|        ALBANIA|\n",
      "| 316.0|        ALGERIA| 316.0|        ALGERIA|\n",
      "| 102.0|        ANDORRA| 102.0|        ANDORRA|\n",
      "| 324.0|         ANGOLA| 324.0|         ANGOLA|\n",
      "| 529.0|       ANGUILLA| 529.0|       ANGUILLA|\n",
      "| 518.0|ANTIGUA-BARBUDA| 518.0|ANTIGUA-BARBUDA|\n",
      "| 687.0|     ARGENTINA | 687.0|     ARGENTINA |\n",
      "| 151.0|        ARMENIA| 151.0|        ARMENIA|\n",
      "| 532.0|          ARUBA| 532.0|          ARUBA|\n",
      "| 438.0|      AUSTRALIA| 438.0|      AUSTRALIA|\n",
      "| 103.0|        AUSTRIA| 103.0|        AUSTRIA|\n",
      "| 152.0|     AZERBAIJAN| 152.0|     AZERBAIJAN|\n",
      "| 512.0|        BAHAMAS| 512.0|        BAHAMAS|\n",
      "| 298.0|        BAHRAIN| 298.0|        BAHRAIN|\n",
      "| 274.0|     BANGLADESH| 274.0|     BANGLADESH|\n",
      "| 513.0|       BARBADOS| 513.0|       BARBADOS|\n",
      "| 104.0|        BELGIUM| 104.0|        BELGIUM|\n",
      "| 581.0|         BELIZE| 581.0|         BELIZE|\n",
      "+------+---------------+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pull in data\n",
    "i94cit_res_codes = spark.read.csv('i94cit_res_codes.csv')\n",
    "i94cit_res_codes = i94cit_res_codes.withColumnRenamed('_c0', 'i94cit')\n",
    "i94cit_res_codes = i94cit_res_codes.withColumnRenamed('_c1', 'i94_City')\n",
    "\n",
    "#Copy df,rename columns, and join\n",
    "res_codes = i94cit_res_codes\n",
    "res_codes = res_codes.withColumnRenamed('i94cit','i94res')\n",
    "res_codes = res_codes.withColumnRenamed('i94_City','i94_Resident')\n",
    "\n",
    "#Join\n",
    "i94cit_res_codes = i94cit_res_codes.join(res_codes,i94cit_res_codes.i94cit==res_codes.i94res)\n",
    "#change data type\n",
    "i94cit_res_codes = i94cit_res_codes.withColumn(\"i94cit\", i94cit_res_codes[\"i94cit\"].cast(\"double\"))\n",
    "i94cit_res_codes = i94cit_res_codes.withColumn(\"i94res\", i94cit_res_codes[\"i94res\"].cast(\"double\"))\n",
    "\n",
    "\n",
    "i94cit_res_codes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+------------+\n",
      "|i94cit|i94_City|i94res|i94_Resident|\n",
      "+------+--------+------+------------+\n",
      "|     0|       0|     0|           0|\n",
      "+------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "i94cit_res_codes.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in i94cit_res_codes.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94port_code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------+\n",
      "|i94port|            location|           Port City|Port State|\n",
      "+-------+--------------------+--------------------+----------+\n",
      "|    ALC|           ALCAN, AK|               ALCAN|        AK|\n",
      "|    ANC|       ANCHORAGE, AK|           ANCHORAGE|        AK|\n",
      "|    BAR|BAKER AAF - BAKER...|BAKER AAF - BAKER...|        AK|\n",
      "|    DAC|   DALTONS CACHE, AK|       DALTONS CACHE|        AK|\n",
      "|    PIZ|DEW STATION PT LA...|DEW STATION PT LA...|        AK|\n",
      "|    DTH|    DUTCH HARBOR, AK|        DUTCH HARBOR|        AK|\n",
      "|    EGL|           EAGLE, AK|               EAGLE|        AK|\n",
      "|    FRB|       FAIRBANKS, AK|           FAIRBANKS|        AK|\n",
      "|    HOM|           HOMER, AK|               HOMER|        AK|\n",
      "|    HYD|           HYDER, AK|               HYDER|        AK|\n",
      "|    JUN|          JUNEAU, AK|              JUNEAU|        AK|\n",
      "|    5KE|       KETCHIKAN, AK|           KETCHIKAN|        AK|\n",
      "|    KET|       KETCHIKAN, AK|           KETCHIKAN|        AK|\n",
      "|    MOS|MOSES POINT INTER...|MOSES POINT INTER...|        AK|\n",
      "|    NIK|         NIKISKI, AK|             NIKISKI|        AK|\n",
      "|    NOM|             NOM, AK|                 NOM|        AK|\n",
      "|    PKC|     POKER CREEK, AK|         POKER CREEK|        AK|\n",
      "|    ORI|  PORT LIONS SPB, AK|      PORT LIONS SPB|        AK|\n",
      "|    SKA|         SKAGWAY, AK|             SKAGWAY|        AK|\n",
      "|    SNP| ST. PAUL ISLAND, AK|     ST. PAUL ISLAND|        AK|\n",
      "+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "i94port_df = spark.read.csv('i94port_code.csv') \n",
    "\n",
    "# Drop empty column\n",
    "i94port_df = i94port_df.drop('_c2')\n",
    "\n",
    "# Remove spaces\n",
    "i94port_df = i94port_df.select('_c0',trim(col('_c1')))\n",
    "\n",
    "# Change column names\n",
    "i94port_df = i94port_df.withColumnRenamed('_c0','i94port')\n",
    "i94port_df = i94port_df.withColumnRenamed('trim(_c1)','location')\n",
    "\n",
    "# get port city and state\n",
    "i94port_df = i94port_df.withColumn('Port City',split(i94port_df.location,',').getItem(0))\n",
    "i94port_df = i94port_df.withColumn('Port State',split(i94port_df.location,',').getItem(1))\n",
    "\n",
    "\n",
    "\n",
    "i94port_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+----------+\n",
      "|i94port|location|Port City|Port State|\n",
      "+-------+--------+---------+----------+\n",
      "|      0|       1|        1|        76|\n",
      "+-------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "i94port_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in i94port_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------+\n",
      "|i94port|            location|           Port City|Port State|\n",
      "+-------+--------------------+--------------------+----------+\n",
      "|    WAS|       WASHINGTON DC|       WASHINGTON DC|      null|\n",
      "|    XXX|NOT REPORTED/UNKNOWN|NOT REPORTED/UNKNOWN|      null|\n",
      "|    888|UNIDENTIFED AIR /...|UNIDENTIFED AIR /...|      null|\n",
      "|    UNK|         UNKNOWN POE|         UNKNOWN POE|      null|\n",
      "|    ZZZ|MEXICO Land (Banc...|MEXICO Land (Banc...|      null|\n",
      "|    CHN|  No PORT Code (CHN)|  No PORT Code (CHN)|      null|\n",
      "|    MAA|           Abu Dhabi|           Abu Dhabi|      null|\n",
      "|    FRG|Collapsed (FOK) 0...|Collapsed (FOK) 0...|      null|\n",
      "|    HRL|Collapsed (HLG) 0...|Collapsed (HLG) 0...|      null|\n",
      "|    ISP|Collapsed (FOK) 0...|Collapsed (FOK) 0...|      null|\n",
      "|    JSJ|Collapsed (SAJ) 0...|Collapsed (SAJ) 0...|      null|\n",
      "|    BUS|Collapsed (BUF) 0...|Collapsed (BUF) 0...|      null|\n",
      "|    IAG|Collapsed (NIA) 0...|Collapsed (NIA) 0...|      null|\n",
      "|    PHN|Collapsed (PHU) 0...|Collapsed (PHU) 0...|      null|\n",
      "|    STN|Collapsed (STR) 0...|Collapsed (STR) 0...|      null|\n",
      "|    VMB|Collapsed (VNB) 0...|Collapsed (VNB) 0...|      null|\n",
      "|    T01|Collapsed (SEA) 0...|Collapsed (SEA) 0...|      null|\n",
      "|    PHF|  No PORT Code (PHF)|  No PORT Code (PHF)|      null|\n",
      "|    DRV|  No PORT Code (DRV)|  No PORT Code (DRV)|      null|\n",
      "|    FTB|  No PORT Code (FTB)|  No PORT Code (FTB)|      null|\n",
      "+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check what null states are\n",
    "i94port_df.where(col('Port State').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### i94addr Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|State Code|            State|\n",
      "+----------+-----------------+\n",
      "|        AL|          ALABAMA|\n",
      "|        AK|           ALASKA|\n",
      "|        AZ|          ARIZONA|\n",
      "|        AR|         ARKANSAS|\n",
      "|        CA|       CALIFORNIA|\n",
      "|        CO|         COLORADO|\n",
      "|        CT|      CONNECTICUT|\n",
      "|        DE|         DELAWARE|\n",
      "|        DC|DIST. OF COLUMBIA|\n",
      "|        FL|          FLORIDA|\n",
      "|        GA|          GEORGIA|\n",
      "|        GU|             GUAM|\n",
      "|        HI|           HAWAII|\n",
      "|        ID|            IDAHO|\n",
      "|        IL|         ILLINOIS|\n",
      "|        IN|          INDIANA|\n",
      "|        IA|             IOWA|\n",
      "|        KS|           KANSAS|\n",
      "|        KY|         KENTUCKY|\n",
      "|        LA|        LOUISIANA|\n",
      "+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "i94addr = spark.read.csv('i94addr.csv')\n",
    "\n",
    "# Change column names\n",
    "i94addr = i94addr.withColumnRenamed('_c0','State Code')\n",
    "i94addr = i94addr.withColumnRenamed('_c1','State')\n",
    "\n",
    "i94addr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Visa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df to merge i94visa and write to csv\n",
    "df_visa = pd.DataFrame({\n",
    "    'i94visa': [1,2,3],\n",
    "    'Visa': ['Business','Pleasure','Student']\n",
    "})\n",
    "\n",
    "# write to csv\n",
    "df_visa.to_csv('visa_data.csv',header=True)\n",
    "# move to s3\n",
    "upload_file(file_name='visa_data.csv' ,bucket='de-capstone',object_name='raw_data/visa_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94visa|    Visa|\n",
      "+-------+--------+\n",
      "|    1.0|Business|\n",
      "|    2.0|Pleasure|\n",
      "|    3.0| Student|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in as spark df\n",
    "df_visa = spark.read.csv('visa_data.csv',header=True)\n",
    "# Drop empty column\n",
    "df_visa = df_visa.drop('_c0')\n",
    "# change data type\n",
    "df_visa = df_visa.withColumn(\"i94visa\", df_visa[\"i94visa\"].cast(\"double\"))\n",
    "\n",
    "df_visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Mode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df to merge i94mode and write to csv\n",
    "df_mode = pd.DataFrame({\n",
    "    'i94mode': [1,2,3,9],\n",
    "    'Mode': ['Air','Sea','Land','Not reported']\n",
    "})\n",
    "\n",
    "#write to csv\n",
    "df_mode.to_csv('mode_data.csv',header=True)\n",
    "#move to s3\n",
    "upload_file(file_name='mode_data.csv' ,bucket='de-capstone',object_name='raw_data/mode_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|i94mode|        Mode|\n",
      "+-------+------------+\n",
      "|    1.0|         Air|\n",
      "|    2.0|         Sea|\n",
      "|    3.0|        Land|\n",
      "|    9.0|Not reported|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in as spark df\n",
    "df_mode = spark.read.csv('mode_data.csv',header=True)\n",
    "# Drop empty column\n",
    "df_mode = df_mode.drop('_c0')\n",
    "# change data type\n",
    "df_mode = df_mode.withColumn(\"i94mode\", df_mode[\"i94mode\"].cast(\"double\"))\n",
    "\n",
    "\n",
    "df_mode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+\n",
      "|      _c0|    cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|  dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+\n",
      "|2027561.0|4084316.0| 2016|     4| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|    61|    2.0|  1.0|2016-04-22|    null| null|      G|      O|   null|      M|   1955|   null|     F|  null|     JL|5.6582674633E10|00782|      WT|\n",
      "|2171295.0|4422636.0| 2016|     4| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|    26|    2.0|  1.0|2016-04-23|     MTR| null|      G|      R|   null|      M|   1990|   null|     M|  null|    *GA| 9.436199593E10|XBLNG|      B2|\n",
      "| 589494.0|1195600.0| 2016|     4| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|    76|    2.0|  1.0|2016-04-07|    null| null|      G|      O|   null|      M|   1940|   null|     M|  null|     LH|5.5780468433E10|00464|      WT|\n",
      "|2631158.0|5291768.0| 2016|     4| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|    25|    2.0|  1.0|2016-04-28|     DOH| null|      G|      O|   null|      M|   1991|   null|     M|  null|     QR| 9.478969603E10|00739|      B2|\n",
      "|3032257.0| 985523.0| 2016|     4| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|    19|    2.0|  1.0|2016-04-06|    null| null|      Z|      K|   null|      M|   1997|   null|     F|  null|   null|4.2322572633E10| LAND|      WT|\n",
      "| 721257.0|1481650.0| 2016|     4| 577.0| 577.0|    ATL|20552.0|    1.0|     GA|20606.0|    51|    2.0|  1.0|2016-04-08|    null| null|      T|      N|   null|      M|   1965|   null|     M|  null|     DL|   7.36852585E8|  910|      B2|\n",
      "|1072780.0|2197173.0| 2016|     4| 245.0| 245.0|    SFR|20556.0|    1.0|     CA|20635.0|    48|    2.0|  1.0|2016-04-12|    null| null|      T|      O|   null|      M|   1968|   null|     F|  null|     CX|   7.86312185E8|  870|      B2|\n",
      "| 112205.0| 232708.0| 2016|     4| 113.0| 135.0|    NYC|20546.0|    1.0|     NY|20554.0|    33|    2.0|  1.0|2016-04-02|    null| null|      G|      O|   null|      M|   1983|   null|     F|  null|     BA|5.5474485033E10|00117|      WT|\n",
      "|2577162.0|5227851.0| 2016|     4| 131.0| 131.0|    CHI|20572.0|    1.0|     IL|20575.0|    39|    2.0|  1.0|2016-04-28|    null| null|      O|      O|   null|      M|   1977|   null|  null|  null|     LX|5.9413424733E10|00008|      WT|\n",
      "|  10930.0|  13213.0| 2016|     4| 116.0| 116.0|    LOS|20545.0|    1.0|     CA|20553.0|    35|    2.0|  1.0|2016-04-01|    null| null|      O|      O|   null|      M|   1981|   null|  null|  null|     AA|5.5449792933E10|00109|      WT|\n",
      "| 617174.0|1230572.0| 2016|     4| 438.0| 438.0|    LOS|20551.0|    1.0|     CA|20565.0|     4|    2.0|  1.0|2016-04-07|    null| null|      G|      O|   null|      M|   2012|   null|     F|  null|     QF|5.5743814633E10|00015|      WT|\n",
      "|2497156.0|5056736.0| 2016|     4| 209.0| 209.0|    PHI|20571.0|    1.0|     HI|20575.0|    72|    2.0|  1.0|2016-04-27|    null| null|      G|      O|   null|      M|   1944|   null|     M|  null|     DL|5.9336618033E10|00598|      WT|\n",
      "|1339656.0|2711583.0| 2016|     4| 148.0| 112.0|    FTL|20559.0|    2.0|   null|20565.0|    54|    2.0|  1.0|2016-04-15|    null| null|      G|      O|   null|      M|   1962|   null|     F|  null|    VES|5.6175860733E10|93724|      WT|\n",
      "|2430322.0|4916639.0| 2016|     4| 260.0| 260.0|    LOS|20570.0|    1.0|     CA|20581.0|    62|    2.0|  1.0|2016-04-26|     MNL| null|      G|      O|   null|      M|   1954|   null|     F|  null|     EK| 9.461277103E10|00215|      B2|\n",
      "| 682005.0|1387607.0| 2016|     4| 148.0| 112.0|    BOS|20552.0|    1.0|     MA|20560.0|    34|    2.0|  1.0|2016-04-08|    null| null|      G|      K|   null|      M|   1982|   null|     F|  null|     AF|5.5833387633E10|00338|      WT|\n",
      "|2938436.0|5960799.0| 2016|     4| 245.0| 245.0|    SAI|20545.0|    1.0|   null|20550.0|    30|    2.0|  1.0|2016-06-15|    null| null|      P|      D|   null|      M|   1986|   null|     M|  3882|     MU|4.4162582033E10|00763|      CP|\n",
      "| 530804.0|1056530.0| 2016|     4| 512.0| 512.0|    NAS|20550.0|    1.0|     GA|20552.0|    21|    2.0|  1.0|2016-04-06|     NSS| null|      G|      O|   null|      M|   1995|   null|     F|  null|     DL| 9.285800163E10|00554|      B2|\n",
      "|1187149.0|2466971.0| 2016|     4| 689.0| 689.0|    FTL|20557.0|    1.0|     FL|20569.0|    43|    2.0|  1.0|2016-04-13|    null| null|      O|      O|   null|      M|   1973|   null|  null|  null|     AD| 9.338490163E10|08704|      B2|\n",
      "|2283033.0|4668286.0| 2016|     4| 746.0| 158.0|    SEA|20568.0|    1.0|     NV|20571.0|    46|    2.0|  1.0|2016-04-24|     MOS| null|      G|      O|   null|      M|   1970|   null|     M|  null|     DL| 9.443560053E10|00143|      B2|\n",
      "|1262082.0|2556646.0| 2016|     4| 260.0| 260.0|    FTL|20558.0|    1.0|     CA|20596.0|    14|    2.0|  1.0|2016-04-14|     MNL| null|      G|      O|   null|      M|   2002|   null|     M|  null|     OZ| 9.353684553E10|00204|      B2|\n",
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df_imm = spark.read.csv('immigration_data_sample.csv', header=True)\n",
    "\n",
    "# Change data types to Double\n",
    "double_type = [\n",
    "    '_c0','cicid','i94yr','i94mon','admnum'\n",
    "]\n",
    "for c in double_type:\n",
    "    df_imm = df_imm.withColumn(c, df_imm[c].cast(DoubleType()))\n",
    "\n",
    "# change data types to integer\n",
    "int_type = [\n",
    "    'i94yr','i94mon','i94bir','biryear'\n",
    "]\n",
    "for c in int_type:\n",
    "    df_imm = df_imm.withColumn(c, df_imm[c].cast(IntegerType()))\n",
    "    \n",
    "#change data to date\n",
    "date_type = [\n",
    "    'dtadfile','dtaddto'\n",
    "]\n",
    "for c in date_type:\n",
    "    df_imm = df_imm.withColumn(c, to_date(df_imm[c],'yyyyMMdd'))\n",
    "\n",
    "df_imm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'double'),\n",
       " ('cicid', 'double'),\n",
       " ('i94yr', 'int'),\n",
       " ('i94mon', 'int'),\n",
       " ('i94cit', 'string'),\n",
       " ('i94res', 'string'),\n",
       " ('i94port', 'string'),\n",
       " ('arrdate', 'string'),\n",
       " ('i94mode', 'string'),\n",
       " ('i94addr', 'string'),\n",
       " ('depdate', 'string'),\n",
       " ('i94bir', 'int'),\n",
       " ('i94visa', 'string'),\n",
       " ('count', 'string'),\n",
       " ('dtadfile', 'date'),\n",
       " ('visapost', 'string'),\n",
       " ('occup', 'string'),\n",
       " ('entdepa', 'string'),\n",
       " ('entdepd', 'string'),\n",
       " ('entdepu', 'string'),\n",
       " ('matflag', 'string'),\n",
       " ('biryear', 'int'),\n",
       " ('dtaddto', 'date'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'double'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Combining Data\n",
    "Here we will combine the data sets for the data lake.  \n",
    "- We will start with the immigration data to combine it on other sets.\n",
    "- We will combine imigration IATA and airport data on IATA code.\n",
    "- We will then combine the demographics/temp data on state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|      _c0|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|2027561.0|4084316.0|2016.0|   4.0| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null| null|      G|      O|   null|      M| 1955.0|07202016|     F|  null|     JL|5.6582674633E10|00782|      WT|\n",
      "|2171295.0|4422636.0|2016.0|   4.0| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR| null|      G|      R|   null|      M| 1990.0|10222016|     M|  null|    *GA| 9.436199593E10|XBLNG|      B2|\n",
      "| 589494.0|1195600.0|2016.0|   4.0| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null| null|      G|      O|   null|      M| 1940.0|07052016|     M|  null|     LH|5.5780468433E10|00464|      WT|\n",
      "|2631158.0|5291768.0|2016.0|   4.0| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH| null|      G|      O|   null|      M| 1991.0|10272016|     M|  null|     QR| 9.478969603E10|00739|      B2|\n",
      "|3032257.0| 985523.0|2016.0|   4.0| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null| null|      Z|      K|   null|      M| 1997.0|07042016|     F|  null|   null|4.2322572633E10| LAND|      WT|\n",
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94visa|    Visa|\n",
      "+-------+--------+\n",
      "|    1.0|Business|\n",
      "|    2.0|Pleasure|\n",
      "|    3.0| Student|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# merge immigration and df_visa data\n",
    "staging_df = df_imm.join(df_visa,df_visa.i94visa==df_imm.i94visa,how='left').select(df_imm[\"*\"],df_visa[\"Visa\"])\n",
    "\n",
    "# merge immigration and df_mode data\n",
    "staging_df = staging_df.join(df_mode,df_mode.i94mode==staging_df.i94mode,how='left').select(staging_df[\"*\"],df_mode[\"Mode\"])\n",
    "\n",
    "# merge immigration and i94port_df data\n",
    "staging_df = staging_df.join(i94port_df,i94port_df.i94port==staging_df.i94port,how='left').select(staging_df[\"*\"],i94port_df[\"Port City\"])\n",
    "staging_df = staging_df.join(i94port_df,i94port_df.i94port==staging_df.i94port,how='left').select(staging_df[\"*\"],i94port_df[\"Port State\"])\n",
    "\n",
    "\n",
    "# merge immigration and i94cit_res_codes data\n",
    "staging_df = staging_df.join(i94cit_res_codes,i94cit_res_codes.i94cit==staging_df.i94cit,how='left').select(staging_df[\"*\"],i94cit_res_codes[\"i94_City\"])\n",
    "staging_df = staging_df.join(i94cit_res_codes,i94cit_res_codes.i94res==staging_df.i94res,how='left').select(staging_df[\"*\"],i94cit_res_codes[\"i94_Resident\"])\n",
    "\n",
    "# merge immigration and i94addr data\n",
    "staging_df = staging_df.join(i94addr,i94addr[\"State Code\"]==staging_df[\"i94addr\"],how='left').select(staging_df[\"*\"],i94addr[\"State\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+----+---------------+----------+-----------+--------------+-------------+\n",
      "|      _c0|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|    Visa|Mode|      Port City|Port State|   i94_City|  i94_Resident|        State|\n",
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+----+---------------+----------+-----------+--------------+-------------+\n",
      "|2027561.0|4084316.0|2016.0|   4.0| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null| null|      G|      O|   null|      M| 1955.0|07202016|     F|  null|     JL|5.6582674633E10|00782|      WT|Pleasure| Air|       HONOLULU|        HI|      JAPAN|         JAPAN|       HAWAII|\n",
      "|2171295.0|4422636.0|2016.0|   4.0| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR| null|      G|      R|   null|      M| 1990.0|10222016|     M|  null|    *GA| 9.436199593E10|XBLNG|      B2|Pleasure| Air|        MCALLEN|        TX|     MEXICO|        MEXICO|        TEXAS|\n",
      "| 589494.0|1195600.0|2016.0|   4.0| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null| null|      G|      O|   null|      M| 1940.0|07052016|     M|  null|     LH|5.5780468433E10|00464|      WT|Pleasure| Air| KAHULUI - MAUI|        HI|       null|       GERMANY|      FLORIDA|\n",
      "|2631158.0|5291768.0|2016.0|   4.0| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH| null|      G|      O|   null|      M| 1991.0|10272016|     M|  null|     QR| 9.478969603E10|00739|      B2|Pleasure| Air|    LOS ANGELES|        CA|      QATAR|         QATAR|   CALIFORNIA|\n",
      "|3032257.0| 985523.0|2016.0|   4.0| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null| null|      Z|      K|   null|      M| 1997.0|07042016|     F|  null|   null|4.2322572633E10| LAND|      WT|Pleasure|Land|      CHAMPLAIN|        NY|     FRANCE|        FRANCE|     NEW YORK|\n",
      "| 721257.0|1481650.0|2016.0|   4.0| 577.0| 577.0|    ATL|20552.0|    1.0|     GA|20606.0|  51.0|    2.0|  1.0|20160408|    null| null|      T|      N|   null|      M| 1965.0|10072016|     M|  null|     DL|   7.36852585E8|  910|      B2|Pleasure| Air|        ATLANTA|        GA|  GUATEMALA|     GUATEMALA|      GEORGIA|\n",
      "|1072780.0|2197173.0|2016.0|   4.0| 245.0| 245.0|    SFR|20556.0|    1.0|     CA|20635.0|  48.0|    2.0|  1.0|20160412|    null| null|      T|      O|   null|      M| 1968.0|10112016|     F|  null|     CX|   7.86312185E8|  870|      B2|Pleasure| Air|  SAN FRANCISCO|        CA| CHINA  PRC|    CHINA  PRC|   CALIFORNIA|\n",
      "| 112205.0| 232708.0|2016.0|   4.0| 113.0| 135.0|    NYC|20546.0|    1.0|     NY|20554.0|  33.0|    2.0|  1.0|20160402|    null| null|      G|      O|   null|      M| 1983.0|06302016|     F|  null|     BA|5.5474485033E10|00117|      WT|Pleasure| Air|       NEW YORK|        NY|     GREECE|UNITED KINGDOM|     NEW YORK|\n",
      "|2577162.0|5227851.0|2016.0|   4.0| 131.0| 131.0|    CHI|20572.0|    1.0|     IL|20575.0|  39.0|    2.0|  1.0|20160428|    null| null|      O|      O|   null|      M| 1977.0|07262016|  null|  null|     LX|5.9413424733E10|00008|      WT|Pleasure| Air|        CHICAGO|        IL|SWITZERLAND|   SWITZERLAND|     ILLINOIS|\n",
      "|  10930.0|  13213.0|2016.0|   4.0| 116.0| 116.0|    LOS|20545.0|    1.0|     CA|20553.0|  35.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1981.0|06292016|  null|  null|     AA|5.5449792933E10|00109|      WT|Pleasure| Air|    LOS ANGELES|        CA|    IRELAND|       IRELAND|   CALIFORNIA|\n",
      "| 617174.0|1230572.0|2016.0|   4.0| 438.0| 438.0|    LOS|20551.0|    1.0|     CA|20565.0|   4.0|    2.0|  1.0|20160407|    null| null|      G|      O|   null|      M| 2012.0|07052016|     F|  null|     QF|5.5743814633E10|00015|      WT|Pleasure| Air|    LOS ANGELES|        CA|  AUSTRALIA|     AUSTRALIA|   CALIFORNIA|\n",
      "|2497156.0|5056736.0|2016.0|   4.0| 209.0| 209.0|    PHI|20571.0|    1.0|     HI|20575.0|  72.0|    2.0|  1.0|20160427|    null| null|      G|      O|   null|      M| 1944.0|07252016|     M|  null|     DL|5.9336618033E10|00598|      WT|Pleasure| Air|   PHILADELPHIA|        PA|      JAPAN|         JAPAN|       HAWAII|\n",
      "|1339656.0|2711583.0|2016.0|   4.0| 148.0| 112.0|    FTL|20559.0|    2.0|   null|20565.0|  54.0|    2.0|  1.0|20160415|    null| null|      G|      O|   null|      M| 1962.0|07132016|     F|  null|    VES|5.6175860733E10|93724|      WT|Pleasure| Sea|FORT LAUDERDALE|        FL|       null|       GERMANY|         null|\n",
      "|2430322.0|4916639.0|2016.0|   4.0| 260.0| 260.0|    LOS|20570.0|    1.0|     CA|20581.0|  62.0|    2.0|  1.0|20160426|     MNL| null|      G|      O|   null|      M| 1954.0|10252016|     F|  null|     EK| 9.461277103E10|00215|      B2|Pleasure| Air|    LOS ANGELES|        CA|PHILIPPINES|   PHILIPPINES|   CALIFORNIA|\n",
      "| 682005.0|1387607.0|2016.0|   4.0| 148.0| 112.0|    BOS|20552.0|    1.0|     MA|20560.0|  34.0|    2.0|  1.0|20160408|    null| null|      G|      K|   null|      M| 1982.0|07062016|     F|  null|     AF|5.5833387633E10|00338|      WT|Pleasure| Air|         BOSTON|        MA|       null|       GERMANY|MASSACHUSETTS|\n",
      "|2938436.0|5960799.0|2016.0|   4.0| 245.0| 245.0|    SAI|20545.0|    1.0|   null|20550.0|  30.0|    2.0|  1.0|20160615|    null| null|      P|      D|   null|      M| 1986.0|04132016|     M|  3882|     MU|4.4162582033E10|00763|      CP|Pleasure| Air|         SAIPAN|       SPN| CHINA  PRC|    CHINA  PRC|         null|\n",
      "| 530804.0|1056530.0|2016.0|   4.0| 512.0| 512.0|    NAS|20550.0|    1.0|     GA|20552.0|  21.0|    2.0|  1.0|20160406|     NSS| null|      G|      O|   null|      M| 1995.0|10052016|     F|  null|     DL| 9.285800163E10|00554|      B2|Pleasure| Air|         NASSAU|   BAHAMAS|    BAHAMAS|       BAHAMAS|      GEORGIA|\n",
      "|1187149.0|2466971.0|2016.0|   4.0| 689.0| 689.0|    FTL|20557.0|    1.0|     FL|20569.0|  43.0|    2.0|  1.0|20160413|    null| null|      O|      O|   null|      M| 1973.0|10122016|  null|  null|     AD| 9.338490163E10|08704|      B2|Pleasure| Air|FORT LAUDERDALE|        FL|     BRAZIL|        BRAZIL|      FLORIDA|\n",
      "|2283033.0|4668286.0|2016.0|   4.0| 746.0| 158.0|    SEA|20568.0|    1.0|     NV|20571.0|  46.0|    2.0|  1.0|20160424|     MOS| null|      G|      O|   null|      M| 1970.0|10232016|     M|  null|     DL| 9.443560053E10|00143|      B2|Pleasure| Air|        SEATTLE|        WA|       null|        RUSSIA|       NEVADA|\n",
      "|1262082.0|2556646.0|2016.0|   4.0| 260.0| 260.0|    FTL|20558.0|    1.0|     CA|20596.0|  14.0|    2.0|  1.0|20160414|     MNL| null|      G|      O|   null|      M| 2002.0|10132016|     M|  null|     OZ| 9.353684553E10|00204|      B2|Pleasure| Air|FORT LAUDERDALE|        FL|PHILIPPINES|   PHILIPPINES|   CALIFORNIA|\n",
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+----+---------------+----------+-----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge immigration and mode data\n",
    "staging_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+\n",
      "|_c0|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|Visa|Mode|Port City|Port State|i94_City|i94_Resident|State|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+\n",
      "|  0|    0|    0|     0|     0|     0|      0|      0|      0|     59|     49|     0|      0|    0|       0|     618|  996|      0|     46|   1000|     46|      0|      0|   141|   965|     33|     0|    8|       0|   0|   0|        0|        27|     127|           0|   67|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "staging_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in staging_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Combine staging, demo, and airport code data\n",
    "final_df = staging_df.join(state_df,lower(state_df[\"State\"])==lower(staging_df[\"State\"]),how='left').drop(state_df[\"State Code\"]).drop(state_df[\"State\"])\n",
    "final_df = final_df.join(df_apt_codes,df_apt_codes[\"iata_code\"]==final_df[\"i94port\"],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+----+-------------+----------+--------+------------+-------------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+--------+-------+-------------------------+------------------+---------------------------------+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "|      _c0|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|    Visa|Mode|    Port City|Port State|i94_City|i94_Resident|        State|AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|   White|  Asian|Black or African-American|Hispanic or Latino|American Indian and Alaska Native|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|State_Code|\n",
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+----+-------------+----------+--------+------------+-------------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+--------+-------+-------------------------+------------------+---------------------------------+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "|2458556.0|5011988.0|2016.0|   4.0| 746.0| 158.0|    NYC|20570.0|    1.0|     NH|20652.0|  74.0|    2.0|  1.0|20160426|     MOS| null|      G|      O|   null|      M| 1942.0|10252016|     F|  null|     SU| 9.460920073E10|00102|      B2|Pleasure| Air|     NEW YORK|        NY|    null|      RUSSIA|NEW HAMPSHIRE| 7.341440525809558|        551115.0|         276890.0|           27365.0|     72530.0|       274225.0|                   2.4|100108.0| 4304.0|                   6896.0|           11962.0|                            558.0| null|         null|                null|        null|     null|       null|      null|        null|    null|     null|      null|                null|      null|\n",
      "|1205276.0|2487808.0|2016.0|   4.0| 111.0| 111.0|    MIA|20558.0|    1.0|     AL|20576.0|  67.0|    2.0|  1.0|20160414|    null| null|      G|      O|   null|      M| 1949.0|07122016|     F|  null|     AF|5.6156579533E10|00090|      WT|Pleasure| Air|        MIAMI|        FL|  FRANCE|      FRANCE|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0| KMIA|large_airport|Miami Internation...|           8|       NA|         US|     US-FL|       Miami|    KMIA|      MIA|       MIA|-80.2906036376953...|        FL|\n",
      "|2355504.0|4756602.0|2016.0|   4.0| 520.0| 520.0|    MIA|20569.0|    1.0|     AL|20626.0|   5.0|    2.0|  1.0|20160425|     BGN| null|      G|      O|   null|      M| 2011.0|10242016|     F|  null|     AA| 9.450089653E10|00982|      B2|Pleasure| Air|        MIAMI|        FL| GRENADA|     GRENADA|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0| KMIA|large_airport|Miami Internation...|           8|       NA|         US|     US-FL|       Miami|    KMIA|      MIA|       MIA|-80.2906036376953...|        FL|\n",
      "|2517012.0|5081528.0|2016.0|   4.0| 254.0| 276.0|    SFR|20571.0|    1.0|     AL|20573.0|  48.0|    1.0|  1.0|20160427|    null| null|      O|      O|   null|      M| 1968.0|07252016|  null|  null|     EV|5.9327444033E10|05510|      WB|Business| Air|SAN FRANCISCO|        CA|    null| SOUTH KOREA|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0|  SFR|       closed|San Fernando Airport|        1168|       NA|         US|     US-CA| Los Angeles|    null|      SFR|      null|    -118.422, 34.289|        CA|\n",
      "| 587246.0|1193133.0|2016.0|   4.0| 148.0| 112.0|    CHI|20551.0|    1.0|     AL|20561.0|  58.0|    2.0|  1.0|20160407|    null| null|      O|      O|   null|      M| 1958.0|07052016|  null|  null|     AF|5.5763314933E10|00682|      WT|Pleasure| Air|      CHICAGO|        IL|    null|     GERMANY|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0| null|         null|                null|        null|     null|       null|      null|        null|    null|     null|      null|                null|      null|\n",
      "+---------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------+----+-------------+----------+--------+------------+-------------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+--------+-------+-------------------------+------------------+---------------------------------+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+-----+-----+-------------------------+------------------+---------------------------------+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "|_c0|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|Visa|Mode|Port City|Port State|i94_City|i94_Resident|State|AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|White|Asian|Black or African-American|Hispanic or Latino|American Indian and Alaska Native|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|State_Code|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+-----+-----+-------------------------+------------------+---------------------------------+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "|  0|    0|    0|     0|     0|     0|      0|      0|      0|     59|     49|     0|      0|    0|       0|     618|  996|      0|     46|   1000|     46|      0|      0|   141|   965|     33|     0|    8|       0|   0|   0|        0|        27|     127|           0|   67|               177|             177|              177|               177|         177|            177|                   177|  177|  177|                      177|               177|                              177|  540| 540| 540|         540|      540|        540|       540|         540|     595|      540|       595|        540|       540|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+-----+-----+-------------------------+------------------+---------------------------------+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "final_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in final_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "![](Immigration_star_schema.png)\n",
    "\n",
    "The Purpose of the data sets are to provide information about to to the individual immigrants.  The only field that tells us about where the individual will be in the United States is the `i94add` field.  This field tells us which U.S. State they are in.  We will use this information to guide how we plan our tables.\n",
    "- Immigration and demographic data will we joined on `State`.\n",
    "- Immigration and airport codes will be joined on `fltno`.\n",
    "- Every individual does not have a matching `fltno` because of land and sea entries.\n",
    "- Every individual does not have a documented `i94addr`, which means they won't have a matching `State`.\n",
    "\n",
    "\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "##### EMR cluster\n",
    "- Set up an AWS EMR cluster\n",
    "    - EMR cluster used: emr-5.20.0\n",
    "    - Cluster needs to have spark and yarn\n",
    "- Move `de-capstone-etl.py` to the EMR cluster\n",
    "- Run `de-capstone-etl.py`\n",
    "    - /usr/bin/spark-submit --master yarn de-capstone-etl.py\n",
    "\n",
    "\n",
    "![](de-pipeline-model.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "import configparser\n",
    "import logging\n",
    "from pyspark.sql.functions import split\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import isnan, when, count, col, regexp_replace, lower, trim, to_date\n",
    "from pyspark.sql.types import DoubleType,IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "########  Airport data\n",
    "# Read in the data here\n",
    "df_apt_codes = spark.read.csv('airport-codes_csv.csv',header=True)\n",
    "\n",
    "# pull just the airports in the US\n",
    "df_apt_codes = df_apt_codes.filter(df_apt_codes.iso_country=='US')\n",
    "\n",
    "# remove iata code == null\n",
    "df_apt_codes = df_apt_codes.filter(df_apt_codes.iata_code.isNotNull())\n",
    "\n",
    "# Get state code from iso_region\n",
    "df_apt_codes = df_apt_codes.withColumn('State_Code',split(df_apt_codes.iso_region,'-').getItem(1))\n",
    "\n",
    "############### Temp data\n",
    "# Check data\n",
    "df_temp = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv',header=True)\n",
    "# Change AverageTemperature to Double\n",
    "df_temp = df_temp.withColumn(\"AverageTemperature\", df_temp[\"AverageTemperature\"].cast(\"double\"))\n",
    "# filter just United states\n",
    "df_temp = df_temp.filter(df_temp.Country == 'United States')\n",
    "# drop nulls\n",
    "df_temp = df_temp.where(col('AverageTemperature').isNotNull())\n",
    "\n",
    "# get average temp for each city\n",
    "avg_temp = df_temp.cube(['City','Latitude','Longitude']).agg({'AverageTemperature':'avg'})\n",
    "avg_temp = avg_temp.na.drop()\n",
    "avg_temp = avg_temp.withColumnRenamed('avg(AverageTemperature)', 'AverageTemperature')\n",
    "\n",
    "################ Demographic data\n",
    "# Read Data\n",
    "df_demo = spark.read.csv('us-cities-demographics.csv',sep = ';',header=True)\n",
    "\n",
    "# Combine temp data\n",
    "df_demo = df_demo.join(avg_temp,avg_temp.City==df_demo.City).select(df_demo[\"*\"],avg_temp[\"AverageTemperature\"])\n",
    "\n",
    "# Break data down to state level\n",
    "state_df = df_demo.cube('State','State Code').agg({'Male Population':'sum',\n",
    "                                     'Female Population':'sum',\n",
    "                                     'Total Population':'sum',\n",
    "                                     'Number of Veterans':'sum',\n",
    "                                     'Foreign-born':'sum',\n",
    "                                     'Average Household Size':'avg',\n",
    "                                     'AverageTemperature':'avg'})\n",
    "state_df = state_df.na.drop()\n",
    "state_df = state_df.withColumnRenamed('avg(AverageTemperature)', 'AverageTemperature')\n",
    "state_df = state_df.withColumnRenamed('avg(Average Household Size)', 'Average Household Size')\n",
    "state_df = state_df.withColumnRenamed('sum(Total Population)', 'Total Population')\n",
    "state_df = state_df.withColumnRenamed('sum(Female Population)', 'Female Population')\n",
    "state_df = state_df.withColumnRenamed('sum(Male Population)', 'Male Population')\n",
    "state_df = state_df.withColumnRenamed('sum(Number of Veterans)', 'Number of Veterans')\n",
    "state_df = state_df.withColumnRenamed('sum(Foreign-born)', 'Foreign-born')\n",
    "\n",
    "# Sum up Races for each state\n",
    "white_df = df_demo.where(df_demo.Race =='White').cube('State').agg({'Count':'sum'})\n",
    "white_df = white_df.withColumnRenamed('sum(Count)', 'White')\n",
    "\n",
    "asian_df = df_demo.where(df_demo.Race =='Asian').cube('State').agg({'Count':'sum'})\n",
    "asian_df = asian_df.withColumnRenamed('sum(Count)', 'Asian')\n",
    "\n",
    "black_df = df_demo.where(df_demo.Race =='Black or African-American').cube('State').agg({'Count':'sum'})\n",
    "black_df = black_df.withColumnRenamed('sum(Count)', 'Black or African-American')\n",
    "\n",
    "hispanic_df = df_demo.where(df_demo.Race =='Hispanic or Latino').cube('State').agg({'Count':'sum'})\n",
    "hispanic_df = hispanic_df.withColumnRenamed('sum(Count)', 'Hispanic or Latino')\n",
    "\n",
    "native_df = df_demo.where(df_demo.Race =='American Indian and Alaska Native').cube('State').agg({'Count':'sum'})\n",
    "native_df = native_df.withColumnRenamed('sum(Count)', 'American Indian and Alaska Native')\n",
    "\n",
    "# Combine race data\n",
    "state_df = state_df.join(white_df,white_df.State==state_df.State).select(state_df[\"*\"],white_df[\"White\"])\n",
    "state_df = state_df.join(asian_df,asian_df.State==state_df.State).select(state_df[\"*\"],asian_df[\"Asian\"])\n",
    "state_df = state_df.join(black_df,black_df.State==state_df.State).select(state_df[\"*\"],black_df[\"Black or African-American\"])\n",
    "state_df = state_df.join(hispanic_df,hispanic_df.State==state_df.State).select(state_df[\"*\"],hispanic_df[\"Hispanic or Latino\"])\n",
    "state_df = state_df.join(native_df,native_df.State==state_df.State).select(state_df[\"*\"],native_df[\"American Indian and Alaska Native\"])\n",
    "\n",
    "#########i94cit_res_codes\n",
    "# pull in data\n",
    "i94cit_res_codes = spark.read.csv('i94cit_res_codes.csv')\n",
    "i94cit_res_codes = i94cit_res_codes.withColumnRenamed('_c0', 'i94cit')\n",
    "i94cit_res_codes = i94cit_res_codes.withColumnRenamed('_c1', 'i94_City')\n",
    "\n",
    "#Copy df,rename columns, and join\n",
    "res_codes = i94cit_res_codes\n",
    "res_codes = res_codes.withColumnRenamed('i94cit','i94res')\n",
    "res_codes = res_codes.withColumnRenamed('i94_City','i94_Resident')\n",
    "\n",
    "#Join\n",
    "i94cit_res_codes = i94cit_res_codes.join(res_codes,i94cit_res_codes.i94cit==res_codes.i94res)\n",
    "#change data type\n",
    "i94cit_res_codes = i94cit_res_codes.withColumn(\"i94cit\", i94cit_res_codes[\"i94cit\"].cast(\"double\"))\n",
    "i94cit_res_codes = i94cit_res_codes.withColumn(\"i94res\", i94cit_res_codes[\"i94res\"].cast(\"double\"))\n",
    "\n",
    "################ i94port_code\n",
    "# Read data\n",
    "i94port_df = spark.read.csv('i94port_code.csv') \n",
    "\n",
    "# Drop empty column\n",
    "i94port_df = i94port_df.drop('_c2')\n",
    "\n",
    "# Remove spaces\n",
    "i94port_df = i94port_df.select('_c0',trim(col('_c1')))\n",
    "\n",
    "# Change column names\n",
    "i94port_df = i94port_df.withColumnRenamed('_c0','i94port')\n",
    "i94port_df = i94port_df.withColumnRenamed('trim(_c1)','location')\n",
    "\n",
    "# get port city and state\n",
    "i94port_df = i94port_df.withColumn('Port City',split(i94port_df.location,',').getItem(0))\n",
    "i94port_df = i94port_df.withColumn('Port State',split(i94port_df.location,',').getItem(1))\n",
    "\n",
    "############## i94addr data\n",
    "#read data\n",
    "i94addr = spark.read.csv('i94addr.csv')\n",
    "\n",
    "# Change column names\n",
    "i94addr = i94addr.withColumnRenamed('_c0','State Code')\n",
    "i94addr = i94addr.withColumnRenamed('_c1','State')\n",
    "\n",
    "############  Visa\n",
    "# read in as spark df\n",
    "df_visa = spark.read.csv('visa_data.csv',header=True)\n",
    "# Drop empty column\n",
    "df_visa = df_visa.drop('_c0')\n",
    "# change data type\n",
    "df_visa = df_visa.withColumn(\"i94visa\", df_visa[\"i94visa\"].cast(\"double\"))\n",
    "############ Mode\n",
    "# read in as spark df\n",
    "df_mode = spark.read.csv('mode_data.csv',header=True)\n",
    "# Drop empty column\n",
    "df_mode = df_mode.drop('_c0')\n",
    "# change data type\n",
    "df_mode = df_mode.withColumn(\"i94mode\", df_mode[\"i94mode\"].cast(\"double\"))\n",
    "\n",
    "############# Immigration data\n",
    "# read data\n",
    "df_imm = spark.read.csv('immigration_data_sample.csv', header=True)\n",
    "\n",
    "# Change data types to Double\n",
    "double_type = [\n",
    "    '_c0','cicid','i94yr','i94mon','admnum'\n",
    "]\n",
    "for c in double_type:\n",
    "    df_imm = df_imm.withColumn(c, df_imm[c].cast(DoubleType()))\n",
    "\n",
    "# change data types to integer\n",
    "int_type = [\n",
    "    'i94yr','i94mon','i94bir','biryear'\n",
    "]\n",
    "for c in int_type:\n",
    "    df_imm = df_imm.withColumn(c, df_imm[c].cast(IntegerType()))\n",
    "    \n",
    "#change data to date\n",
    "date_type = [\n",
    "    'dtadfile','dtaddto'\n",
    "]\n",
    "for c in date_type:\n",
    "    df_imm = df_imm.withColumn(c, to_date(df_imm[c],'yyyyMMdd'))\n",
    "\n",
    "########### Combining data\n",
    "# merge immigration and df_visa data\n",
    "staging_df = df_imm.join(df_visa,df_visa.i94visa==df_imm.i94visa,how='left').select(df_imm[\"*\"],df_visa[\"Visa\"])\n",
    "\n",
    "# merge immigration and df_mode data\n",
    "staging_df = staging_df.join(df_mode,df_mode.i94mode==staging_df.i94mode,how='left').select(staging_df[\"*\"],df_mode[\"Mode\"])\n",
    "\n",
    "# merge immigration and i94port_df data\n",
    "staging_df = staging_df.join(i94port_df,i94port_df.i94port==staging_df.i94port,how='left').select(staging_df[\"*\"],i94port_df[\"Port City\"])\n",
    "staging_df = staging_df.join(i94port_df,i94port_df.i94port==staging_df.i94port,how='left').select(staging_df[\"*\"],i94port_df[\"Port State\"])\n",
    "\n",
    "\n",
    "# merge immigration and i94cit_res_codes data\n",
    "staging_df = staging_df.join(i94cit_res_codes,i94cit_res_codes.i94cit==staging_df.i94cit,how='left').select(staging_df[\"*\"],i94cit_res_codes[\"i94_City\"])\n",
    "staging_df = staging_df.join(i94cit_res_codes,i94cit_res_codes.i94res==staging_df.i94res,how='left').select(staging_df[\"*\"],i94cit_res_codes[\"i94_Resident\"])\n",
    "\n",
    "# merge immigration and i94addr data\n",
    "staging_df = staging_df.join(i94addr,i94addr[\"State Code\"]==staging_df[\"i94addr\"],how='left').select(staging_df[\"*\"],i94addr[\"State\"])\n",
    "\n",
    "\n",
    "# Combine staging, demo, and airport code data\n",
    "final_df = staging_df.join(state_df,lower(state_df[\"State\"])==lower(staging_df[\"State\"]),how='left').drop(state_df[\"State Code\"]).drop(state_df[\"State\"])\n",
    "final_df = final_df.join(df_apt_codes,df_apt_codes[\"iata_code\"]==final_df[\"i94port\"],how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+\n",
      "|      _c0|    cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|  dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+\n",
      "|2027561.0|4084316.0| 2016|     4| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|    61|    2.0|  1.0|2016-04-22|    null| null|      G|      O|   null|      M|   1955|   null|     F|  null|     JL|5.6582674633E10|00782|      WT|\n",
      "|2171295.0|4422636.0| 2016|     4| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|    26|    2.0|  1.0|2016-04-23|     MTR| null|      G|      R|   null|      M|   1990|   null|     M|  null|    *GA| 9.436199593E10|XBLNG|      B2|\n",
      "| 589494.0|1195600.0| 2016|     4| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|    76|    2.0|  1.0|2016-04-07|    null| null|      G|      O|   null|      M|   1940|   null|     M|  null|     LH|5.5780468433E10|00464|      WT|\n",
      "|2631158.0|5291768.0| 2016|     4| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|    25|    2.0|  1.0|2016-04-28|     DOH| null|      G|      O|   null|      M|   1991|   null|     M|  null|     QR| 9.478969603E10|00739|      B2|\n",
      "|3032257.0| 985523.0| 2016|     4| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|    19|    2.0|  1.0|2016-04-06|    null| null|      Z|      K|   null|      M|   1997|   null|     F|  null|   null|4.2322572633E10| LAND|      WT|\n",
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+---------+--------+-------------------------+------------------+---------------------------------+\n",
      "|    State|State Code|AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|    White|   Asian|Black or African-American|Hispanic or Latino|American Indian and Alaska Native|\n",
      "+---------+----------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+---------+--------+-------------------------+------------------+---------------------------------+\n",
      "|     Utah|        UT| 9.898601558734654|       2780420.0|        1376780.0|           87315.0|    426475.0|      1403640.0|    3.2325000000000004| 447902.0| 30346.0|                  12828.0|          122622.0|                          11466.0|\n",
      "|Minnesota|        MN|5.8729834447628075|       4119955.0|        2064815.0|          163265.0|    725230.0|      2055140.0|    2.4633333333333334| 562708.0|101867.0|                 154086.0|           73905.0|                          17268.0|\n",
      "|     Ohio|        OH| 11.33790573901892|      1.501329E7|        7728170.0|          753265.0|   1323230.0|      7285120.0|    2.2728571428571427|1831672.0|133451.0|                1061736.0|          175677.0|                          40291.0|\n",
      "| Arkansas|        AR|16.398790531226602|       1404090.0|         709310.0|           85435.0|    114765.0|       694780.0|    2.3199999999999994| 171142.0| 13130.0|                  92533.0|           21035.0|                           3019.0|\n",
      "|   Oregon|        OR| 9.772739535594242|       4800890.0|        2433760.0|          242605.0|    601060.0|      2367130.0|    2.4866666666666664| 819715.0| 81535.0|                  55316.0|          117834.0|                          28027.0|\n",
      "+---------+----------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+---------+--------+-------------------------+------------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|State_Code|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|       NA|         US|     US-FL|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|        FL|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|       NA|         US|     US-AK|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|        AK|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|       NA|         US|     US-CO|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|        CO|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515|       NA|         US|     US-TX| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|        TX|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       NA|         US|     US-MA|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|        MA|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apt_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+--------+----+-------------+----------+--------+------------+-------------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+--------+-------+-------------------------+------------------+---------------------------------+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "|      _c0|    cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|  dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|         admnum|fltno|visatype|    Visa|Mode|    Port City|Port State|i94_City|i94_Resident|        State|AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|   White|  Asian|Black or African-American|Hispanic or Latino|American Indian and Alaska Native|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|State_Code|\n",
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+--------+----+-------------+----------+--------+------------+-------------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+--------+-------+-------------------------+------------------+---------------------------------+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "|2458556.0|5011988.0| 2016|     4| 746.0| 158.0|    NYC|20570.0|    1.0|     NH|20652.0|    74|    2.0|  1.0|2016-04-26|     MOS| null|      G|      O|   null|      M|   1942|   null|     F|  null|     SU| 9.460920073E10|00102|      B2|Pleasure| Air|     NEW YORK|        NY|    null|      RUSSIA|NEW HAMPSHIRE| 7.341440525809558|        551115.0|         276890.0|           27365.0|     72530.0|       274225.0|                   2.4|100108.0| 4304.0|                   6896.0|           11962.0|                            558.0| null|         null|                null|        null|     null|       null|      null|        null|    null|     null|      null|                null|      null|\n",
      "|1205276.0|2487808.0| 2016|     4| 111.0| 111.0|    MIA|20558.0|    1.0|     AL|20576.0|    67|    2.0|  1.0|2016-04-14|    null| null|      G|      O|   null|      M|   1949|   null|     F|  null|     AF|5.6156579533E10|00090|      WT|Pleasure| Air|        MIAMI|        FL|  FRANCE|      FRANCE|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0| KMIA|large_airport|Miami Internation...|           8|       NA|         US|     US-FL|       Miami|    KMIA|      MIA|       MIA|-80.2906036376953...|        FL|\n",
      "|2355504.0|4756602.0| 2016|     4| 520.0| 520.0|    MIA|20569.0|    1.0|     AL|20626.0|     5|    2.0|  1.0|2016-04-25|     BGN| null|      G|      O|   null|      M|   2011|   null|     F|  null|     AA| 9.450089653E10|00982|      B2|Pleasure| Air|        MIAMI|        FL| GRENADA|     GRENADA|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0| KMIA|large_airport|Miami Internation...|           8|       NA|         US|     US-FL|       Miami|    KMIA|      MIA|       MIA|-80.2906036376953...|        FL|\n",
      "|2517012.0|5081528.0| 2016|     4| 254.0| 276.0|    SFR|20571.0|    1.0|     AL|20573.0|    48|    1.0|  1.0|2016-04-27|    null| null|      O|      O|   null|      M|   1968|   null|  null|  null|     EV|5.9327444033E10|05510|      WB|Business| Air|SAN FRANCISCO|        CA|    null| SOUTH KOREA|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0|  SFR|       closed|San Fernando Airport|        1168|       NA|         US|     US-CA| Los Angeles|    null|      SFR|      null|    -118.422, 34.289|        CA|\n",
      "| 587246.0|1193133.0| 2016|     4| 148.0| 112.0|    CHI|20551.0|    1.0|     AL|20561.0|    58|    2.0|  1.0|2016-04-07|    null| null|      O|      O|   null|      M|   1958|   null|  null|  null|     AF|5.5763314933E10|00682|      WT|Pleasure| Air|      CHICAGO|        IL|    null|     GERMANY|      ALABAMA|16.911072296533963|       3994580.0|        2095865.0|          283715.0|    187600.0|      1898715.0|                   2.3|340932.0|20102.0|                 437303.0|           31704.0|                           7167.0| null|         null|                null|        null|     null|       null|      null|        null|    null|     null|      null|                null|      null|\n",
      "+---------+---------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+----------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+---------------+-----+--------+--------+----+-------------+----------+--------+------------+-------------+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+--------+-------+-------------------------+------------------+---------------------------------+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+-----+-----+-------------------------+------------------+---------------------------------+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "|_c0|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|gender|insnum|airline|admnum|fltno|visatype|Visa|Mode|Port City|Port State|i94_City|i94_Resident|State|AverageTemperature|Total Population|Female Population|Number of Veterans|Foreign-born|Male Population|Average Household Size|White|Asian|Black or African-American|Hispanic or Latino|American Indian and Alaska Native|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|State_Code|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+-----+-----+-------------------------+------------------+---------------------------------+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "|  0|    0|    0|     0|     0|     0|      0|      0|      0|     59|     49|     0|      0|    0|     618|  996|      0|     46|   1000|     46|      0|   141|   965|     33|     0|    8|       0|   0|   0|        0|        27|     127|           0|   67|               177|             177|              177|               177|         177|            177|                   177|  177|  177|                      177|               177|                              177|  540| 540| 540|         540|      540|        540|       540|         540|     595|      540|       595|        540|       540|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+----+----+---------+----------+--------+------------+-----+------------------+----------------+-----------------+------------------+------------+---------------+----------------------+-----+-----+-------------------------+------------------+---------------------------------+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for nulls\n",
    "final_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in final_df.columns if c not in {'dtadfile','dtaddto'}]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Data quality checks will include the following:\n",
    "\n",
    "- Check primary key for nulls.\n",
    "    - We want to make sure we are able to match the data on primary keys.\n",
    "    - `state_df.State`\n",
    "    - `staging_df.admnum`\n",
    "    - `df_apt_codes.ident`\n",
    "- Check number of entries in staging_df and final_df are greater than 1 million.\n",
    "    - We want to make sure we have enough data to pass the 1 million records mark.\n",
    "\n",
    "Not all immigrants have a US address or a US i94port so not everyone will have state data or US airport info attached to the final data frame.  Data quality checks will be performed on both the final data frame as well as the three main data frames prior to joining them.  \n",
    "\n",
    "#### Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "assert state_df.where(col('State').isNull()).count()==0,'Null values in state_df.State'\n",
    "assert df_imm.where(col('admnum').isNull()).count()==0,'Null values in df_imm.admnum'\n",
    "assert df_apt_codes.where(col('ident').isNull()).count()==0,'Null values in df_apt_codes.ident'\n",
    "assert final_df.count()>1000000,'final_df has less than 1 million entries'\n",
    "assert df_imm.count()>1000000,'df_imm has less than 1 million entries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ident': 'Identity code of airport',\n",
       " 'type': 'Type of airport(size/type)',\n",
       " 'name': 'Name of airport',\n",
       " 'elevation_ft': 'elevation of airport in feet',\n",
       " 'continent': 'Continent of airport',\n",
       " 'iso_country': 'Country of airport',\n",
       " 'iso_region': 'Region, country and state',\n",
       " 'municipality': 'Municipality/location',\n",
       " 'gps_code': 'GPS code',\n",
       " 'iata_code': '3 Letter abbreviation of airport',\n",
       " 'local_code': 'Local code',\n",
       " 'coordinates': 'Longitude and latitude location',\n",
       " 'State_Code': 'State abbreviation'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Airport dictionary\n",
    "apt_dict = {\n",
    "    'ident':'Identity code of airport',\n",
    "    'type':'Type of airport(size/type)',\n",
    "    'name':'Name of airport',\n",
    "    'elevation_ft':'elevation of airport in feet',\n",
    "    'continent':'Continent of airport',\n",
    "    'iso_country':'Country of airport',\n",
    "    'iso_region':'Region, country and state',\n",
    "    'municipality':'Municipality/location',\n",
    "    'gps_code':'GPS code',\n",
    "    'iata_code':'3 Letter abbreviation of airport',\n",
    "    'local_code':'Local code',\n",
    "    'coordinates':'Longitude and latitude location',\n",
    "    'State_Code':'State abbreviation'\n",
    "}\n",
    "apt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_c0': 'Indenifier',\n",
       " 'cicid': 'ID of some sort',\n",
       " 'i94yr': 'Year of immigration application',\n",
       " 'i94mon': 'Month of immigration application',\n",
       " 'i94cit': 'City of application code',\n",
       " 'i94res': 'City of residence code',\n",
       " 'i94port': 'Port of entry',\n",
       " 'arrdate': 'Arrival date',\n",
       " 'i94mode': 'Mode of entry code',\n",
       " 'i94addr': 'US address state of expected residence',\n",
       " 'depdate': 'Departure date',\n",
       " 'i94bir': 'Age',\n",
       " 'i94visa': 'Visa type code',\n",
       " 'count': 'Number of people on application',\n",
       " 'dtadfile': 'Date application recieved',\n",
       " 'visapost': 'Visa post of application',\n",
       " 'occup': 'Occupation',\n",
       " 'entdepa': 'Admitted date ',\n",
       " 'entdepd': 'Departed date',\n",
       " 'entdepu': 'Either apprehended, overstayed, adjusted to perm residence',\n",
       " 'matflag': 'Match of arrival and departure records',\n",
       " 'biryear': '4 digit birth year',\n",
       " 'dtaddto': 'Date to which admitted to U.S.',\n",
       " 'gender': 'Gender',\n",
       " 'insnum': 'INS number',\n",
       " 'airline': 'Airline used to arrive in U.S.',\n",
       " 'admnum': 'Admission Number',\n",
       " 'fltno': 'Flight number of Airline used to arrive in U.S.',\n",
       " 'visatype': 'Class of admission legally admitting the non-immigrant to temporarily stay in U.S.',\n",
       " 'Visa': 'Type of visa',\n",
       " 'Mode': 'Mode of entry',\n",
       " 'Port City': 'Port city',\n",
       " 'Port State': 'Port state',\n",
       " 'i94_City': 'City of application',\n",
       " 'i94_Resident': 'City of residence',\n",
       " 'State': 'US address state of expected residence'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Immigation dictionary\n",
    "imm_dict = {\n",
    "    '_c0':'Indenifier',\n",
    "    'cicid':'ID of some sort',\n",
    "    'i94yr':'Year of immigration application',\n",
    "    'i94mon':'Month of immigration application',\n",
    "    'i94cit':'City of application code',\n",
    "    'i94res':'City of residence code',\n",
    "    'i94port':'Port of entry',\n",
    "    'arrdate':'Arrival date',\n",
    "    'i94mode':'Mode of entry code',\n",
    "    'i94addr':'US address state of expected residence',\n",
    "    'depdate':'Departure date',\n",
    "    'i94bir':'Age',\n",
    "    'i94visa':'Visa type code',\n",
    "    'count':'Number of people on application',\n",
    "    'dtadfile':'Date application recieved',\n",
    "    'visapost':'Visa post of application',\n",
    "    'occup':'Occupation',\n",
    "    'entdepa':'Admitted date ',\n",
    "    'entdepd':'Departed date',\n",
    "    'entdepu':'Either apprehended, overstayed, adjusted to perm residence',\n",
    "    'matflag':'Match of arrival and departure records',\n",
    "    'biryear':'4 digit birth year',\n",
    "    'dtaddto':'Date to which admitted to U.S.',\n",
    "    'gender':'Gender',\n",
    "    'insnum':'INS number',\n",
    "    'airline':'Airline used to arrive in U.S.',\n",
    "    'admnum':'Admission Number',\n",
    "    'fltno':'Flight number of Airline used to arrive in U.S.',\n",
    "    'visatype':'Class of admission legally admitting the non-immigrant to temporarily stay in U.S.',\n",
    "    'Visa':'Type of visa',\n",
    "    'Mode':'Mode of entry',\n",
    "    'Port City':'Port city',\n",
    "    'Port State':'Port state',\n",
    "    'i94_City':'City of application',\n",
    "    'i94_Resident':'City of residence',\n",
    "    'State':'US address state of expected residence'\n",
    "}\n",
    "imm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'State': 'State',\n",
       " 'State Code': 'State abbreviation',\n",
       " 'AverageTemperature': 'Average temperature of state',\n",
       " 'Total Population': 'Total population',\n",
       " 'Female Population': 'Female population',\n",
       " 'Number of Veterans': 'Number of veterans',\n",
       " 'Foreign-born': 'Number of foreign born',\n",
       " 'Male Population': 'Male population',\n",
       " 'Average Household Size': 'Average household size',\n",
       " 'White': 'White population',\n",
       " 'Asian': 'Asian population',\n",
       " 'Black or African-American': 'Black population',\n",
       " 'Hispanic or Latino': 'Hispanic population',\n",
       " 'American Indian and Alaska Native': 'Indian and alaskan population'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# State dictionary\n",
    "state_dict = {\n",
    "    'State':'State',\n",
    "    'State Code':'State abbreviation',\n",
    "    'AverageTemperature':'Average temperature of state',\n",
    "    'Total Population':'Total population',\n",
    "    'Female Population':'Female population',\n",
    "    'Number of Veterans':'Number of veterans',\n",
    "    'Foreign-born':'Number of foreign born',\n",
    "    'Male Population':'Male population',\n",
    "    'Average Household Size':'Average household size',\n",
    "    'White':'White population',\n",
    "    'Asian':'Asian population',\n",
    "    'Black or African-American':'Black population',\n",
    "    'Hispanic or Latino':'Hispanic population',\n",
    "    'American Indian and Alaska Native':'Indian and alaskan population'\n",
    "}\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "Rational for tools and technologies:\n",
    "- We used an EMR cluster and spark to be able to handle the data.  Immigration data had over 3 million records.\n",
    "- Spark requires less resources, as compared to pandas, and allows us to deal with the amount of data.\n",
    "\n",
    "Updating methods:\n",
    "- The immigration data in this project is from April 2016.  The data is then partitioned by State when being written to S3.\n",
    "- If we need to update the data with new data for different months/years we would partition the data by year and month as well.\n",
    "- If we need to update the month with new data we would need to read the data in and overwrite the data after is has been cleaned.\n",
    "\n",
    "Potential problems in different scenarios:\n",
    "- The data was increased by 100x.\n",
    "    - We could easily add additional nodes to the EMR cluster and use optimized nodes for RAM.\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - We could add airflow integration to automate and schedule the program.\n",
    "    - We could also use other AWS data pipeline solutions ex: Lambda functions.\n",
    "- The database needed to be accessed by 100+ people.\n",
    "    - The current s3 solution can be accessed by 100+ people.  The data stored for a data warehouse can be moved to a AWS Redshift cluster for people to query if needed.\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
